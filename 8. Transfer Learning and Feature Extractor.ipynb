{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "swp5cafJVx48"
      },
      "source": [
        "# **Keras Cats vs Dogs - Feature Extraction**\n",
        "\n",
        "---\n",
        "\n",
        "Now we will use a pretrained network as a feature extractor. We'll then use those feautres as the input for our Logistic Regression Clasifier.\n",
        "1. Download and Explore our data\n",
        "2. Load our pretrained VGG16 Model\n",
        "3. Extract our Features using VGG16\n",
        "4. Train a LR Classifier using those features\n",
        "5. Test some inferences "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-QylEpnmV3YG"
      },
      "source": [
        "## **1. Download and Explore our data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0_FPvfTUUJRG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e36f4445-4e6a-4646-d36e-ed14f651dc92"
      },
      "source": [
        "!wget https://moderncomputervision.s3.eu-west-2.amazonaws.com/dogs-vs-cats.zip\n",
        "!unzip -q dogs-vs-cats.zip\n",
        "!unzip -q train.zip\n",
        "!unzip -q test1.zip "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-10-17 04:52:19--  https://moderncomputervision.s3.eu-west-2.amazonaws.com/dogs-vs-cats.zip\n",
            "Resolving moderncomputervision.s3.eu-west-2.amazonaws.com (moderncomputervision.s3.eu-west-2.amazonaws.com)... 52.95.150.170\n",
            "Connecting to moderncomputervision.s3.eu-west-2.amazonaws.com (moderncomputervision.s3.eu-west-2.amazonaws.com)|52.95.150.170|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 851576689 (812M) [application/zip]\n",
            "Saving to: ‘dogs-vs-cats.zip’\n",
            "\n",
            "dogs-vs-cats.zip    100%[===================>] 812.13M  84.6MB/s    in 9.6s    \n",
            "\n",
            "2022-10-17 04:52:29 (84.2 MB/s) - ‘dogs-vs-cats.zip’ saved [851576689/851576689]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nuVd2pOTT8Ln",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "59dc5430-954e-4eef-bd17-eca702e7929e"
      },
      "source": [
        "import os\n",
        "import tqdm\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd \n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.applications import VGG16, imagenet_utils\n",
        "from tensorflow.keras.preprocessing.image import array_to_img, img_to_array, load_img\n",
        "\n",
        "filenames = os.listdir(\"./train\")\n",
        "\n",
        "categories = []\n",
        "\n",
        "for filename in filenames:\n",
        "    category = filename.split('.')[0]\n",
        "    if category == 'dog':\n",
        "        categories.append(1)\n",
        "    else:\n",
        "        categories.append(0)\n",
        "\n",
        "df = pd.DataFrame({'filename': filenames, 'class': categories})\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        filename  class\n",
              "0   dog.4296.jpg      1\n",
              "1   cat.6011.jpg      0\n",
              "2   dog.6246.jpg      1\n",
              "3    dog.823.jpg      1\n",
              "4  cat.11175.jpg      0"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-3bea09c9-3783-4c76-8e81-900979137894\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>filename</th>\n",
              "      <th>class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>dog.4296.jpg</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>cat.6011.jpg</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>dog.6246.jpg</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>dog.823.jpg</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>cat.11175.jpg</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3bea09c9-3783-4c76-8e81-900979137894')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-3bea09c9-3783-4c76-8e81-900979137894 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-3bea09c9-3783-4c76-8e81-900979137894');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S4n50QkWV53I"
      },
      "source": [
        "## **2. Load our pretrained VGG16 Model** "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CttEJB0lUJ-w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aad9b6ac-04fc-47b5-d28c-900089e6f67f"
      },
      "source": [
        "model = VGG16(weights=\"imagenet\", include_top=False)\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"vgg16\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, None, None, 3)]   0         \n",
            "                                                                 \n",
            " block1_conv1 (Conv2D)       (None, None, None, 64)    1792      \n",
            "                                                                 \n",
            " block1_conv2 (Conv2D)       (None, None, None, 64)    36928     \n",
            "                                                                 \n",
            " block1_pool (MaxPooling2D)  (None, None, None, 64)    0         \n",
            "                                                                 \n",
            " block2_conv1 (Conv2D)       (None, None, None, 128)   73856     \n",
            "                                                                 \n",
            " block2_conv2 (Conv2D)       (None, None, None, 128)   147584    \n",
            "                                                                 \n",
            " block2_pool (MaxPooling2D)  (None, None, None, 128)   0         \n",
            "                                                                 \n",
            " block3_conv1 (Conv2D)       (None, None, None, 256)   295168    \n",
            "                                                                 \n",
            " block3_conv2 (Conv2D)       (None, None, None, 256)   590080    \n",
            "                                                                 \n",
            " block3_conv3 (Conv2D)       (None, None, None, 256)   590080    \n",
            "                                                                 \n",
            " block3_pool (MaxPooling2D)  (None, None, None, 256)   0         \n",
            "                                                                 \n",
            " block4_conv1 (Conv2D)       (None, None, None, 512)   1180160   \n",
            "                                                                 \n",
            " block4_conv2 (Conv2D)       (None, None, None, 512)   2359808   \n",
            "                                                                 \n",
            " block4_conv3 (Conv2D)       (None, None, None, 512)   2359808   \n",
            "                                                                 \n",
            " block4_pool (MaxPooling2D)  (None, None, None, 512)   0         \n",
            "                                                                 \n",
            " block5_conv1 (Conv2D)       (None, None, None, 512)   2359808   \n",
            "                                                                 \n",
            " block5_conv2 (Conv2D)       (None, None, None, 512)   2359808   \n",
            "                                                                 \n",
            " block5_conv3 (Conv2D)       (None, None, None, 512)   2359808   \n",
            "                                                                 \n",
            " block5_pool (MaxPooling2D)  (None, None, None, 512)   0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 14,714,688\n",
            "Trainable params: 14,714,688\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3OllIRxpWIDl"
      },
      "source": [
        "## **3. Extract our Features using VGG16**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nl3uTgwMUa0A"
      },
      "source": [
        "# We're taking the output of the last CONV-POOL layer (see below). \n",
        "# The output shape at this layer is (7 x 7 x 512)\n",
        "batch_size = 32\n",
        "image_labels = []\n",
        "image_features = []\n",
        "image_paths = filenames\n",
        "labels = categories\n",
        "\n",
        "# loop over each batch\n",
        "for i in range(0, len(image_paths)//batch_size):\n",
        "  \n",
        "  # extract our batches\n",
        "  batch_paths = image_paths[i:i + batch_size]\n",
        "  batch_labels = labels[i:i + batch_size]\n",
        "  batch_images = []\n",
        "\n",
        "  # iterate over each image and extract our image features\n",
        "  for image_path in batch_paths:\n",
        "\n",
        "    # load images using Keras's load_img() and resize to 224 x 244\n",
        "    image = load_img('train/'+image_path, target_size = (224, 224))\n",
        "    image = img_to_array(image)\n",
        "\n",
        "    # We expand the dimensions and then subtract the mean RGB pixel intensity of ImageNet\n",
        "    # using the imagenet_utils.preprocess_input() function\n",
        "    image = np.expand_dims(image, axis=0)\n",
        "    image = imagenet_utils.preprocess_input(image)\n",
        "\n",
        "    # append our image features to our batch list\n",
        "    batch_images.append(image)\n",
        "\n",
        "  # we take our batch of images and get in the right format with vstack\n",
        "  batch_images = np.vstack(batch_images)\n",
        "\n",
        "  # we then that batch and run it throuhg our predict function\n",
        "  features = model.predict(batch_images, batch_size = batch_size)\n",
        "\n",
        "  # we then take the output shape 7x7x512 and flatten it\n",
        "  features = np.reshape(features,(-1, 7*7*512))\n",
        "\n",
        "  # store our features and corresponding labels\n",
        "  image_features.append(features)\n",
        "  image_labels.append(batch_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L68oOLdOWSI2"
      },
      "source": [
        "# lets look at the image imageFeatures\n",
        "print(image_features[0].shape)\n",
        "image_features[0]\n",
        "print(\"\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "59JlS6sIWMrC"
      },
      "source": [
        "## **4. Train a LR Classifier using those features**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e3JcyWaFUrJ5"
      },
      "source": [
        "# First let's store our extracted feature info in a format that can loaded directly by sklearn. \n",
        "# Take our list of batches and reduce the dimension so that it's now a list 25088 features x 25000 rows (25000 x 1 for our labels)\n",
        "imageLabels_data =  [lb for label_batch in image_labels for lb in label_batch]\n",
        "imageFeatures_data = [feature for feature_batch in image_features for feature in feature_batch]\n",
        "\n",
        "# Convert to numpy arrays\n",
        "image_labels_data = np.array(imageLabels_data)\n",
        "image_features_data = np.array(imageFeatures_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VB4cv_7EUvqX"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "# Split our model into a test and training dataset to train our LR classifier\n",
        "X_train, X_test, y_train, y_test = train_test_split(image_features_data,\n",
        "                                                    image_labels_data, \n",
        "                                                    test_size = 0.2, \n",
        "                                                    random_state = 7)\n",
        "\n",
        "glm = LogisticRegression(C = 0.1)\n",
        "glm.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J2oSsyfX0DAc"
      },
      "source": [
        "# Get Accruacy on the 20% we split from our training dataset\n",
        "accuracy = glm.score(X_test, y_test)\n",
        "print(f'Accuracy on validation set using Logistic Regression: {accuracy*100}%')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wkBGaa7oWjPi"
      },
      "source": [
        "## **5. Test some inferences**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IB5XK9lCU2np"
      },
      "source": [
        "image_names_test = os.listdir(\"./test1\")\n",
        "image_paths_test = [\"./test1/\"+ x for x in image_names_test]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6dkkZXExU3Pm"
      },
      "source": [
        "import random \n",
        "\n",
        "test_sample = random.sample(image_paths_test, 12)\n",
        "\n",
        "def test_img():\n",
        "    result_lst = []\n",
        "    for path in test_sample:\n",
        "        image = load_img(path, target_size=(224, 224))\n",
        "        image = img_to_array(image)\n",
        "        image = np.expand_dims(image, axis=0)\n",
        "        image = imagenet_utils.preprocess_input(image)\n",
        "        features = model.predict(image)\n",
        "        features = np.reshape(features,(-1,7*7*512))\n",
        "        result = glm.predict(features)\n",
        "        result = 'dog' if float(result) >0.5 else 'cat'\n",
        "        result_lst.append(result)\n",
        "    return result_lst"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fzjNsGtZU3lg"
      },
      "source": [
        "# get test predictions from all models\n",
        "pred_results = test_img()\n",
        "pred_results"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f1Ct2g6uU3n6"
      },
      "source": [
        "plt.figure(figsize=(15, 15))\n",
        "\n",
        "for i in range(0, 12):\n",
        "    plt.subplot(4, 3, i+1)\n",
        "    result = pred_results[i]\n",
        "    img = test_sample[i]\n",
        "    image = load_img(img, target_size=(256,256))\n",
        "    plt.text(72, 248, f'Feature Extractor CNN: {result}', color='lightgreen',fontsize= 12, bbox=dict(facecolor='black', alpha=0.9))\n",
        "    plt.imshow(image)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}